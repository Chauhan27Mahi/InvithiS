{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPnwBV1m0pvqvhovOSNwQNw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"source":["!pip install git+https://github.com/openai/whisper.git\n","!pip install pytube\n","import whisper\n","from pytube import YouTube\n","import os\n","import torch\n","from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n","import random"],"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T3Cl545pedv3","executionInfo":{"status":"ok","timestamp":1720159196348,"user_tz":-330,"elapsed":27846,"user":{"displayName":"Mahi Chauhan","userId":"14910846049759000963"}},"outputId":"624f97b8-5387-43cd-cdac-73541a07bd1f"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/openai/whisper.git\n","  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-_kq1wxy6\n","  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-_kq1wxy6\n","  Resolved https://github.com/openai/whisper.git to commit ba3f3cd54b0e5b8ce1ab3de13e32122d0d5f98ab\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (0.58.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (1.25.2)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.3.0+cu121)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (4.66.4)\n","Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (10.1.0)\n","Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (0.7.0)\n","Requirement already satisfied: triton<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.3.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton<3,>=2.0.0->openai-whisper==20231117) (3.15.4)\n","Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20231117) (0.41.1)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2024.5.15)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2.31.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (1.12.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (2023.6.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (2.20.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (12.1.105)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->openai-whisper==20231117) (12.5.82)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2024.6.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20231117) (2.1.5)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper==20231117) (1.3.0)\n","Collecting pytube\n","  Downloading pytube-15.0.0-py3-none-any.whl (57 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pytube\n","Successfully installed pytube-15.0.0\n"]}]},{"cell_type":"code","source":["def download_audio(video_url):\n","    yt = YouTube(video_url)\n","    audio_stream = yt.streams.filter(only_audio=True).first()\n","    output_file = audio_stream.download(filename_prefix='audio_')\n","    return output_file\n","\n","# Function to transcribe audio from downloaded file\n","def transcribe_audio_whisper(audio_file):\n","    model = whisper.load_model(\"base\")\n","    result = model.transcribe(audio_file, fp16=False)\n","    return result[\"text\"]\n","\n","# Function to generate questions\n","def generate_questions(text):\n","    tokenizer = AutoTokenizer.from_pretrained(\"valhalla/t5-base-qg-hl\")\n","    model = AutoModelForSeq2SeqLM.from_pretrained(\"valhalla/t5-base-qg-hl\")\n","\n","    input_text = \"generate questions: \" + text\n","    inputs = tokenizer.encode(input_text, return_tensors='pt')\n","\n","    output_sequences = model.generate(\n","        input_ids=inputs,\n","        max_length=512,\n","        num_beams=5,\n","        no_repeat_ngram_size=2,\n","        num_return_sequences=5,\n","        early_stopping=True\n","    )\n","\n","    questions = [tokenizer.decode(sequence, skip_special_tokens=True) for sequence in output_sequences]\n","    return questions\n","\n","# Function to generate MCQ\n","def generate_mcq(transcribed_text, question):\n","    sentences = transcribed_text.split('.')\n","    correct_answer = random.choice(sentences).strip()\n","\n","    incorrect_answers = random.sample(sentences, 3)\n","    incorrect_answers = [ans.strip() for ans in incorrect_answers if ans.strip() != correct_answer][:3]\n","\n","    while len(incorrect_answers) < 3:\n","        incorrect_answers.append(\"This is a dummy incorrect answer.\")\n","\n","    options = incorrect_answers + [correct_answer]\n","    random.shuffle(options)\n","\n","    return {\n","        \"question\": question,\n","        \"options\": options,\n","        \"correct_answer\": correct_answer\n","    }\n","\n","# Main function to process video URL and generate MCQs\n","def generate_transcript_and_mcqs(video_url):\n","    audio_file = download_audio(video_url)\n","    transcribed_text = transcribe_audio_whisper(audio_file)\n","\n","    questions = generate_questions(transcribed_text)\n","    mcqs = [generate_mcq(transcribed_text, question) for question in questions]\n","\n","    os.remove(audio_file)\n","\n","    return mcqs\n","\n","# Function to display MCQs and get user answers\n","def display_mcqs(mcqs):\n","    for i, mcq in enumerate(mcqs):\n","        print(f\"Q{i+1}: {mcq['question']}\")\n","        for idx, option in enumerate(mcq['options']):\n","            print(f\"  {chr(65 + idx)}. {option}\")\n","\n","        user_answer = input(\"Your answer (A, B, C, D): \").strip().upper()\n","        correct_option = chr(65 + mcq['options'].index(mcq['correct_answer']))\n","\n","        if user_answer == correct_option:\n","            print(\"Correct!\")\n","        else:\n","            print(f\"Incorrect! The correct answer is {correct_option}. {mcq['correct_answer']}\")\n","        print(\"\\n\")\n","\n","# Give the URL code\n","video_url = input(\"Enter YouTube URL: \")  # Taking YouTube URL as input from the user\n","mcqs = generate_transcript_and_mcqs(video_url)\n","display_mcqs(mcqs)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ADXd5k-tdwoA","executionInfo":{"status":"ok","timestamp":1720159653342,"user_tz":-330,"elapsed":173739,"user":{"displayName":"Mahi Chauhan","userId":"14910846049759000963"}},"outputId":"5fcb91e8-22fd-48dd-aea1-0a5fd2e3d320"},"execution_count":5,"outputs":[{"name":"stdout","output_type":"stream","text":["Enter YouTube URL: https://youtu.be/BqqfQnyjmgg?si=MagfG4_eQgSEEYnT\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Token indices sequence length is longer than the specified maximum sequence length for this model (787 > 512). Running this sequence through the model will result in indexing errors\n"]},{"output_type":"stream","name":"stdout","text":["Q1: What is the purpose of transfer learning?\n","  A. Earth was pre-trained this way using the English Wikipedia and had 11,000 published books\n","  B. This is because pre-training models are usually trained on large amounts of data, but provides a model with statistical understanding of the language used during pre-training\n","  C. Now let's say you want to train a model B for a different task\n","  D. For instance, when we defined it in the Bert model earlier, we removed the ad-let classified mass quotes and replaced it with a classifier with two outputs, since our task at two labels\n","Your answer (A, B, C, D): B\n","Incorrect! The correct answer is D. For instance, when we defined it in the Bert model earlier, we removed the ad-let classified mass quotes and replaced it with a classifier with two outputs, since our task at two labels\n","\n","\n","Q2: How is transfer learning applied to a model?\n","  A. Training on the right it's fine training a pre-training model\n","  B. Models are frequently trained on image net, but are set containing 1\n","  C. OpenAI also studied the bias in the prediction of its GPT-3 model, which was between using the guess and next word objectives\n","  D. In natural language processing, transfer learning is a bit more recent\n","Your answer (A, B, C, D): A\n","Correct!\n","\n","\n","Q3: What is transfer learning?\n","  A. Now let's say you want to train a model B for a different task\n","  B. In this example we are training a belt model on the task of recognizing if two sentences are similar or not\n","  C. To be as efficient as possible, the pre-trained model used should be as similar as possible to the task its fine tuned on\n","  D. In practice, transfer learning is applied on a given model by throwing away its head, that is, its last layers focused on the pre-training objective, and replacing it with a new, randomly initialized ed suitable for the task attempt\n","Your answer (A, B, C, D): b\n","Correct!\n","\n","\n","Q4: What does transfer learning do to a model?\n","  A. As we can see, using transfer learning on the pre-training model yields better results\n","  B. To be as efficient as possible, the pre-trained model used should be as similar as possible to the task its fine tuned on\n","  C. In this example we are training a belt model on the task of recognizing if two sentences are similar or not\n","  D. One option would be to train the model from scratch\n","Your answer (A, B, C, D): a\n","Correct!\n","\n","\n","Q5: What is a transfer learning concept?\n","  A. The model A will be trained specifically for task A\n","  B. Training like this, unlabeled data, is called supervised learning\n","  C. 22 millions of photo images\n","  D. What is transfer learning? The idea of transfer learning is to leverage the knowledge acquired by a moral train with lots of data on another task\n","Your answer (A, B, C, D): d\n","Incorrect! The correct answer is C. 22 millions of photo images\n","\n","\n"]}]}]}